{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PDFAlto versions comparison and evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9252aa15de011c08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:05:17.148112Z",
     "start_time": "2025-05-05T16:05:17.104713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def compute_statistics(input_pdfalto_A, input_pdfalto_B):\n",
    "    common_subdirs = extract_common_repositories(input_pdfalto_A, input_pdfalto_B)\n",
    "    print(\"Found the following corpus directories: \", common_subdirs)\n",
    "\n",
    "    documents = {}\n",
    "    for corpus in common_subdirs:\n",
    "        print(corpus)\n",
    "        skipped = []\n",
    "        processed = {}\n",
    "        documents[corpus] = {\n",
    "            'processed': processed,\n",
    "            'skipped': skipped,\n",
    "            'pdfalto_A': input_pdfalto_A,\n",
    "            'pdfalto_B': input_pdfalto_B,\n",
    "            'average': {\n",
    "                'pdfalto_A': 0,\n",
    "                'pdfalto_B': 0,\n",
    "                'diff': 0,\n",
    "                'perc': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for root, dirs, files in tqdm(os.walk(os.path.join(input_pdfalto_A, corpus))):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".txt\"):\n",
    "                    continue\n",
    "\n",
    "                # Find the directory relative to the corpus directory\n",
    "                file_id = os.path.relpath(os.path.join(root, file),\n",
    "                                          os.path.join(input_pdfalto_A, corpus)).replace(\".txt\", \"\")\n",
    "\n",
    "                with open(os.path.join(root, file), \"r\") as f:\n",
    "                    txt_pdfalto_A = f.read()\n",
    "\n",
    "                related_B_file = os.path.join(input_pdfalto_B, corpus, file_id + \".txt\")\n",
    "                if not os.path.exists(related_B_file):\n",
    "                    skipped.append((file_id, related_B_file))\n",
    "                    continue\n",
    "\n",
    "                with open(related_B_file, \"r\") as f:\n",
    "                    txt_pdfalto_B = f.read()\n",
    "\n",
    "                txt_pdfalto_A_spaces = ' '.join(txt_pdfalto_A.split())\n",
    "                txt_pdfalto_B_spaces = ' '.join(txt_pdfalto_B.split())\n",
    "\n",
    "                txt_pdfalto_A_spaces_len = len(txt_pdfalto_A_spaces)\n",
    "                txt_pdfalto_B_spaces_len = len(txt_pdfalto_B_spaces)\n",
    "                diff_chars_spaces = txt_pdfalto_A_spaces_len - txt_pdfalto_B_spaces_len\n",
    "\n",
    "                txt_pdfalto_A_no_spaces = ''.join(txt_pdfalto_A.split())\n",
    "                txt_pdfalto_B_no_spaces = ''.join(txt_pdfalto_B.split())\n",
    "\n",
    "                txt_pdfalto_A_len = len(txt_pdfalto_A_no_spaces)\n",
    "                txt_pdfalto_B_len = len(txt_pdfalto_B_no_spaces)\n",
    "                diff_chars_no_spaces = txt_pdfalto_A_len - txt_pdfalto_B_len\n",
    "\n",
    "                txt_pdfalto_A_token_len = len(txt_pdfalto_A_spaces.split())\n",
    "                txt_pdfalto_B_token_len = len(txt_pdfalto_B_spaces.split())\n",
    "                diff_tokens = txt_pdfalto_A_token_len - txt_pdfalto_B_token_len\n",
    "\n",
    "                # related_grobid_file = related_grobid_file.replace(\"tei.xml\", \"diff.txt\")\n",
    "                # # save files\n",
    "                # with open(related_grobid_file, \"w\") as f:\n",
    "                #     f.write(txt_grobid_spaces)\n",
    "                #     f.write(\"\\n\")\n",
    "                #     f.write(txt_pdfalto_spaces)\n",
    "\n",
    "                processed[file_id] = {\n",
    "                    'pdfalto_A': len(txt_pdfalto_A),\n",
    "                    'pdfalto_A_tokens': txt_pdfalto_A_token_len,\n",
    "                    'pdfalto_B': len(txt_pdfalto_B),\n",
    "                    'pdfalto_B_tokens': txt_pdfalto_B_token_len,\n",
    "                    'stats': {\n",
    "                        'diff_chars_no_spaces': diff_chars_no_spaces,\n",
    "                        'diff_chars_spaces': diff_chars_spaces,\n",
    "                        'diff_tokens': diff_tokens\n",
    "                    }\n",
    "                }\n",
    "        print(f\"Processed documents {len(processed)}\")\n",
    "        print(f\"Skipped documents {len(skipped)}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def extract_common_repositories(input_pdfalto, input_grobid):\n",
    "    # Find the subdirectories in the input corpora\n",
    "    subdirs_pdfalto = [x for x in os.listdir(input_pdfalto)]\n",
    "    print(f\"Found the following corpus directories for PDFAlto A: {subdirs_pdfalto}\")\n",
    "    subdirs_grobid = [x for x in os.listdir(input_grobid)]\n",
    "    print(f\"Found the following corpus directories for PDFAlto B: {subdirs_grobid}\")\n",
    "    # find the common subdirectories\n",
    "    common_subdirs = [x for x in subdirs_grobid if x in subdirs_pdfalto]\n",
    "    return common_subdirs\n",
    "\n",
    "documents_output = {}\n"
   ],
   "id": "c76832531bd7c98f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison",
   "id": "5d4cc0c23b3b8445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:45.450284Z",
     "start_time": "2025-05-05T16:05:17.226036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdfalto_A = \"/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/output_pdfalto/lin64-0.5\"\n",
    "pdfalto_B = \"/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/output_pdfalto/output-pdfalto-8cf749a-250505\"\n",
    "\n",
    "documents_output[\"standard\"] = compute_statistics(pdfalto_A, pdfalto_B)"
   ],
   "id": "5929a39f01ff90fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto A: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for PDFAlto B: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "PLOS_1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:11, 88.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents 1000\n",
      "Skipped documents 0\n",
      "eLife_984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "985it [00:21, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents 984\n",
      "Skipped documents 0\n",
      "biorxiv-10k-test-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [00:30, 65.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents 2000\n",
      "Skipped documents 0\n",
      "PMC_sample_1943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:24, 78.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents 1943\n",
      "Skipped documents 0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:45.873257Z",
     "start_time": "2025-05-05T16:06:45.830345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_averages(documents_):\n",
    "    for process_type, process in documents_.items():\n",
    "        for corpus, documents in process.items():\n",
    "            total_pdfalto_A = sum([d['pdfalto_A'] for d in documents['processed'].values()])\n",
    "            total_pdfalto_B = sum([d['pdfalto_B'] for d in documents['processed'].values()])\n",
    "            total_diff_chars_no_spaces = sum([d['stats']['diff_chars_no_spaces'] for d in documents['processed'].values()])\n",
    "            total_diff_chars_spaces = sum([d['stats']['diff_chars_spaces'] for d in documents['processed'].values()])\n",
    "            matching_documents_chars_no_spaces = sum([d['stats']['diff_chars_no_spaces'] == 0 for d in documents['processed'].values()])\n",
    "            matching_documents_chars_spaces = sum([d['stats']['diff_chars_spaces'] == 0 for d in documents['processed'].values()])\n",
    "            total_diff_tokens = sum([d['stats']['diff_tokens'] for d in documents['processed'].values()])\n",
    "            matching_documents_tokens = sum([d['stats']['diff_tokens'] == 0 for d in documents['processed'].values()])\n",
    "\n",
    "            documents['average']['pdfalto_A'] = total_pdfalto_A / len(documents['processed'])\n",
    "            documents['average']['pdfalto_B'] = total_pdfalto_B / len(documents['processed'])\n",
    "            documents['average']['diff_chars_no_spaces'] = total_diff_chars_no_spaces / len(documents['processed'])\n",
    "            documents['average']['diff_chars_spaces'] = total_diff_chars_spaces / len(documents['processed'])\n",
    "            documents['average']['diff_tokens'] = total_diff_tokens / len(documents['processed'])\n",
    "            documents['average']['matching_documents_chars_spaces'] = matching_documents_chars_spaces\n",
    "            documents['average']['matching_documents_chars_no_spaces'] = matching_documents_chars_no_spaces\n",
    "            documents['average']['matching_documents_tokens'] = matching_documents_tokens\n",
    "\n",
    "    return documents_\n",
    "\n",
    "\n",
    "documents_output_with_average = compute_averages(documents_output)"
   ],
   "id": "27c3e979c72d58b1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results",
   "id": "92922999340648a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Matching documents\n",
   "id": "5018e40e17213ba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:46.271976Z",
     "start_time": "2025-05-05T16:06:46.111473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Chars/Tokens\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output_with_average.items():\n",
    "    process_data = [\"Chars (no spaces)\"]\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['matching_documents_chars_no_spaces'])\n",
    "    aggregated_data.append(process_data)\n",
    "    process_data = [\"Chars (spaces)\"]\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['matching_documents_chars_spaces'])\n",
    "    aggregated_data.append(process_data)\n",
    "    process_data = [\"Tokens\"]\n",
    "    for corpus, documents in process.items():\n",
    "        process_data.append(documents['average']['matching_documents_tokens'])\n",
    "    aggregated_data.append(process_data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(aggregated_data, columns=columns)"
   ],
   "id": "e30d2f3d89607614",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Chars/Tokens  PLOS_1000  eLife_984  biorxiv-10k-test-2000  \\\n",
       "0  Chars (no spaces)        748        511                   1548   \n",
       "1     Chars (spaces)        391        187                    740   \n",
       "2             Tokens        404        214                    761   \n",
       "\n",
       "   PMC_sample_1943  \n",
       "0             1451  \n",
       "1              868  \n",
       "2              892  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chars/Tokens</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chars (no spaces)</td>\n",
       "      <td>748</td>\n",
       "      <td>511</td>\n",
       "      <td>1548</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chars (spaces)</td>\n",
       "      <td>391</td>\n",
       "      <td>187</td>\n",
       "      <td>740</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tokens</td>\n",
       "      <td>404</td>\n",
       "      <td>214</td>\n",
       "      <td>761</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Average difference",
   "id": "75865c148e8fb4f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:46.501805Z",
     "start_time": "2025-05-05T16:06:46.474633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Process type\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output_with_average.items():\n",
    "    process_data = [\"Chars (no spaces)\"]\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['diff_chars_no_spaces'])\n",
    "    aggregated_data.append(process_data)\n",
    "    process_data = [\"Chars (spaces)\"]\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['diff_chars_spaces'])\n",
    "    aggregated_data.append(process_data)\n",
    "    process_data = [\"Tokens\"]\n",
    "    for corpus, documents in process.items():\n",
    "        process_data.append(documents['average']['diff_tokens'])\n",
    "    aggregated_data.append(process_data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(aggregated_data, columns=columns)"
   ],
   "id": "6290ffccf7a4c38c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Process type  PLOS_1000  eLife_984  biorxiv-10k-test-2000  \\\n",
       "0  Chars (no spaces)     -3.196  -7.197154                -4.0065   \n",
       "1     Chars (spaces)     -4.641 -12.759146                -8.4745   \n",
       "2             Tokens     -1.445  -5.561992                -4.4680   \n",
       "\n",
       "   PMC_sample_1943  \n",
       "0        -2.457025  \n",
       "1        -3.888832  \n",
       "2        -1.431806  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process type</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chars (no spaces)</td>\n",
       "      <td>-3.196</td>\n",
       "      <td>-7.197154</td>\n",
       "      <td>-4.0065</td>\n",
       "      <td>-2.457025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chars (spaces)</td>\n",
       "      <td>-4.641</td>\n",
       "      <td>-12.759146</td>\n",
       "      <td>-8.4745</td>\n",
       "      <td>-3.888832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tokens</td>\n",
       "      <td>-1.445</td>\n",
       "      <td>-5.561992</td>\n",
       "      <td>-4.4680</td>\n",
       "      <td>-1.431806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysis specific documents",
   "id": "201380166577b233"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:48.598066Z",
     "start_time": "2025-05-05T16:06:48.453041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for process_type, process in documents_output.items():\n",
    "    print(process_type)\n",
    "    for corpus, documents in process.items():\n",
    "        print(f\"\\t{corpus}\")\n",
    "        sorted_by_diff = sorted(\n",
    "            documents['processed'].items(),\n",
    "            key=lambda x: x[1]['stats']['diff_chars_no_spaces']\n",
    "        )\n",
    "\n",
    "        sorted_by_diff_tokens = sorted(\n",
    "            documents['processed'].items(),\n",
    "            key=lambda x: x[1]['stats']['diff_tokens']\n",
    "        )\n",
    "\n",
    "        output_neg = [f\"\\n\\t\\t\\t -{item[0]}: {item[1]['stats']['diff_chars_no_spaces']}\" for item in sorted_by_diff[:2]]\n",
    "        print(\"\".join(output_neg))\n",
    "        output_pos = [f\"\\n\\t\\t\\t -{item[0]}: {item[1]['stats']['diff_chars_no_spaces']}\" for item in sorted_by_diff[-2:]]\n",
    "        print(\"\".join(output_pos))\n",
    "        # print(f\"Files to check:\\n {[item[0] for item in sorted_by_diff_tokens[:2]]}\")\n"
   ],
   "id": "f4da94821bc2d8e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pone.0278112/pone.0278112: -317\n",
      "\t\t\t -pone.0278819/pone.0278819: -217\n",
      "\n",
      "\t\t\t -pone.0278186/pone.0278186: 0\n",
      "\t\t\t -pone.0278971/pone.0278971: 0\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -63910/elife-63910-v2: -169\n",
      "\t\t\t -08954/elife-08954-v1: -144\n",
      "\n",
      "\t\t\t -44795/elife-44795-v2: 0\n",
      "\t\t\t -32143/elife-32143-v2: 0\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -172486v1/172486v1: -422\n",
      "\t\t\t -187518v1/187518v1: -290\n",
      "\n",
      "\t\t\t -286617v1/286617v1: 0\n",
      "\t\t\t -438572v1/438572v1: 0\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Comput_Math_Methods_Med_2011_Mar_8_2011_790721/CMMM2011-790721: -60\n",
      "\t\t\t -BMC_Clin_Pathol_2011_May_10_11_6/1472-6890-11-6: -59\n",
      "\n",
      "\t\t\t -Clin_Res_Cardiol_2011_May_4_100(5)_433-438/392_2010_Article_261: 0\n",
      "\t\t\t -Scand_J_Food_Nutr_2007_Sep_51(3)_91-99/FNR-51-091: 0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:49.722481Z",
     "start_time": "2025-05-05T16:06:49.708844Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a34716d113f49f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:06:50.321336Z",
     "start_time": "2025-05-05T16:06:50.317942Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "640dcd89ec09a866",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
