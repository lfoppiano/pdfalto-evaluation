{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Grobid / PDFAlto evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9252aa15de011c08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:37.865432Z",
     "start_time": "2025-03-13T05:50:28.981514Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tabulate",
   "id": "1743c31ae4423bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /Users/lfoppiano/anaconda3/envs/jupyter/lib/python3.10/site-packages (0.9.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:38.034849Z",
     "start_time": "2025-03-13T05:50:37.959500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import os\n",
    "\n",
    "\n",
    "def compute_statistics(input_pdfalto, input_grobid):\n",
    "    common_subdirs = extract_common_repositories(input_pdfalto, input_grobid)\n",
    "    print(\"Found the following corpus directories: \", common_subdirs)\n",
    "\n",
    "    documents = {}\n",
    "    for corpus in tqdm(common_subdirs, desc=\"Processing corpora\", unit=\"corpus\"):\n",
    "        # tqdm.write(corpus)\n",
    "        skipped = []\n",
    "        processed = {}\n",
    "        documents[corpus] = {\n",
    "            'processed': processed,\n",
    "            'skipped': skipped,\n",
    "            'average': {\n",
    "                'pdfalto': 0,\n",
    "                'grobid': 0,\n",
    "                'diff': 0,\n",
    "                'perc': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for root, dirs, files in os.walk(os.path.join(input_pdfalto, corpus)):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".txt\"):\n",
    "                    continue\n",
    "\n",
    "                # Find the directory relative to the corpus directory\n",
    "                file_id = os.path.relpath(os.path.join(root, file),\n",
    "                                          os.path.join(input_pdfalto, corpus)).replace(\".txt\", \"\")\n",
    "\n",
    "                with open(os.path.join(root, file), \"r\") as f:\n",
    "                    txt_pdfalto = f.read()\n",
    "\n",
    "                related_grobid_file = os.path.join(input_grobid, corpus, file_id + \".grobid.tei.xml\")\n",
    "                if not os.path.exists(related_grobid_file):\n",
    "                    skipped.append((file_id, related_grobid_file))\n",
    "                    continue\n",
    "\n",
    "                txt_grobid = \"\"\n",
    "                grobid_file_txt = related_grobid_file.replace(\".grobid.tei.xml\", \".grobid.txt\")\n",
    "                if os.path.exists(grobid_file_txt):\n",
    "                    with open(grobid_file_txt, \"r\") as f:\n",
    "                        txt_grobid = f.read()\n",
    "                else:\n",
    "                    # Parse xml and extract all text\n",
    "                    root = ElementTree().parse(related_grobid_file)\n",
    "                    txt_grobid = ' '.join(root.itertext())\n",
    "\n",
    "                    with open(grobid_file_txt, \"w\") as f:\n",
    "                        f.write(txt_grobid)\n",
    "\n",
    "                # Strip out spaces, tabs and newlines\n",
    "                txt_pdfalto_spaces = ' '.join(txt_pdfalto.split())\n",
    "                txt_grobid_spaces = ' '.join(txt_grobid.split())\n",
    "\n",
    "                # We keep examples with breaklines\n",
    "                txt_pdfalto_spaces_breaklines = \"\\n\".join([line.strip() for line in txt_pdfalto.split(\"\\n\") if line.strip()])\n",
    "                txt_grobid_spaces_breaklines = \"\\n\".join([line.strip() for line in txt_grobid.split(\"\\n\") if line.strip()])\n",
    "\n",
    "                txt_grobid_spaces_len = len(txt_pdfalto_spaces)\n",
    "                txt_pdfalto_spaces_len = len(txt_grobid_spaces)\n",
    "\n",
    "                txt_pdfalto_no_spaces = ''.join(txt_pdfalto.split())\n",
    "                txt_grobid_no_spaces = ''.join(txt_grobid.split())\n",
    "\n",
    "                txt_pdfalto_len = len(txt_pdfalto_no_spaces)\n",
    "                txt_grobid_len = len(txt_grobid_no_spaces)\n",
    "                diff_chars = txt_pdfalto_len - txt_grobid_len\n",
    "\n",
    "                txt_pdfalto_token_len = len(txt_pdfalto_spaces.split())\n",
    "                txt_grobid_token_len = len(txt_grobid_spaces.split())\n",
    "                diff_tokens = txt_pdfalto_token_len - txt_grobid_token_len\n",
    "\n",
    "                diff_grobid_file = related_grobid_file.replace(\".grobid.tei.xml\", \".diff.grobid.txt\")\n",
    "                if not os.path.exists(diff_grobid_file):\n",
    "                    # save files\n",
    "                    with open(diff_grobid_file, \"w\") as f:\n",
    "                        f.write(txt_grobid_spaces_breaklines)\n",
    "\n",
    "                diff_pdfalto_file = related_grobid_file.replace(\".grobid.tei.xml\", \".diff.pdfalto.txt\")\n",
    "                if not os.path.exists(diff_pdfalto_file):\n",
    "                    # save files\n",
    "                    with open(diff_pdfalto_file, \"w\") as f:\n",
    "                        f.write(txt_pdfalto_spaces_breaklines)\n",
    "\n",
    "                processed[file_id] = {\n",
    "                    'pdfalto': len(txt_pdfalto),\n",
    "                    'pdfalto_tokens': txt_pdfalto_token_len,\n",
    "                    'grobid': len(txt_grobid),\n",
    "                    'grobid_tokens': txt_grobid_token_len,\n",
    "                    'stats': {\n",
    "                        'diff': diff_chars,\n",
    "                        'diff_tokens': diff_tokens,\n",
    "                        'perc': (1 - (txt_grobid_len / txt_pdfalto_len)) * 100 if txt_pdfalto_len > 0 else 0.0,\n",
    "                        'perc_tokens': (1 - (\n",
    "                                txt_grobid_token_len / txt_pdfalto_token_len)) * 100 if txt_pdfalto_token_len > 0 else 0.0\n",
    "                    }\n",
    "                }\n",
    "        # tqdm.write(f\"Processed documents {len(processed)}\")\n",
    "        # tqdm.write(f\"Skipped documents {len(skipped)}\")\n",
    "\n",
    "    for corpus, docs in documents.items():\n",
    "        print(f\"Corpus: {corpus}, Processed {len(docs['processed'])}, skipped: {len(docs['skipped'])}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def extract_common_repositories(input_pdfalto, input_grobid):\n",
    "    # Find the subdirectories in the input corpora\n",
    "    subdirs_pdfalto = [x for x in os.listdir(input_pdfalto)]\n",
    "    print(f\"Found the following corpus directories for PDFAlto: {subdirs_pdfalto}\")\n",
    "    subdirs_grobid = [x for x in os.listdir(input_grobid)]\n",
    "    print(f\"Found the following corpus directories for Grobid: {subdirs_grobid}\")\n",
    "    # find the common subdirectories\n",
    "    common_subdirs = [x for x in subdirs_grobid if x in subdirs_pdfalto]\n",
    "    return common_subdirs\n",
    "\n",
    "#\n",
    "# negative_diff_documents = list(filter(lambda d: d[1]['stats']['diff'] < 0, documents[corpus].items()))\n",
    "# print(f\"Negative diff documents {len(negative_diff_documents)}\")\n",
    "# print(json.dumps(\n",
    "#     negative_diff_documents, indent=4\n",
    "# ))"
   ],
   "id": "c76832531bd7c98f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:38.208755Z",
     "start_time": "2025-03-13T05:50:38.196002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_CORPORA_PDFALTO = \"/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/pdfalto/lin64-0.5\"\n",
    "documents_output = {}"
   ],
   "id": "8dfb71341c448d76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:38.242717Z",
     "start_time": "2025-03-13T05:50:38.238902Z"
    }
   },
   "cell_type": "code",
   "source": "INPUT_CORPORA_GROBID = \"/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/grobid.v6_2\"",
   "id": "1b25bf9bca62c1b1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results Grobid (normal)",
   "id": "5d4cc0c23b3b8445"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:40.032670Z",
     "start_time": "2025-03-13T05:50:38.612311Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"standard\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/normal-sentences\")",
   "id": "5929a39f01ff90fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/grobid.v6_2/normal-sentences'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m documents_output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstandard\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_statistics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_CORPORA_PDFALTO\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mINPUT_CORPORA_GROBID\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/normal-sentences\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m, in \u001B[0;36mcompute_statistics\u001B[0;34m(input_pdfalto, input_grobid)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_statistics\u001B[39m(input_pdfalto, input_grobid):\n\u001B[0;32m----> 7\u001B[0m     common_subdirs \u001B[38;5;241m=\u001B[39m \u001B[43mextract_common_repositories\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_pdfalto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_grobid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound the following corpus directories: \u001B[39m\u001B[38;5;124m\"\u001B[39m, common_subdirs)\n\u001B[1;32m     10\u001B[0m     documents \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[0;32mIn[2], line 116\u001B[0m, in \u001B[0;36mextract_common_repositories\u001B[0;34m(input_pdfalto, input_grobid)\u001B[0m\n\u001B[1;32m    114\u001B[0m subdirs_pdfalto \u001B[38;5;241m=\u001B[39m [x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(input_pdfalto)]\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound the following corpus directories for PDFAlto: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubdirs_pdfalto\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 116\u001B[0m subdirs_grobid \u001B[38;5;241m=\u001B[39m [x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_grobid\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound the following corpus directories for Grobid: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubdirs_grobid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m# find the common subdirectories\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Volumes/ExtremePro/sciencialab/dimensions/pdfalto-evaluation/grobid.v6_2/normal-sentences'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:40.881457Z",
     "start_time": "2025-03-11T04:31:07.093614Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"standard (paragraphs)\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/normal-paragraphs\")",
   "id": "35fe2c35e9ec5433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:48<00:00, 12.21s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results Grobid (light process)\n",
   "id": "ed217b12aaa90451"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.201068Z",
     "start_time": "2025-03-11T04:31:55.964088Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"light\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/light-sentences\")",
   "id": "9101d7b62ff11217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:52<00:00, 13.11s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.246280Z",
     "start_time": "2025-03-11T04:32:48.447081Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"light (paragraphs)\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/light-paragraphs\")",
   "id": "249097be84afe2d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:48<00:00, 12.17s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grobid results (normal + collectDiscardedText)",
   "id": "8ffff46b91db0fcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.280540Z",
     "start_time": "2025-03-11T04:33:37.194809Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"standard + collectDiscardedText\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/normal-discarded-sentences\")",
   "id": "bbeaca877ec79751",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:57<00:00, 14.43s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.292114Z",
     "start_time": "2025-03-11T04:34:34.982748Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"standard + collectDiscardedText (paragraphs)\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/normal-discarded-paragraphs\")",
   "id": "da1f1ad05498fab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:49<00:00, 12.41s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Result Grobid (lightweight) + collectDiscardedText",
   "id": "73a0c9413cfee28c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.316892Z",
     "start_time": "2025-03-11T04:35:24.739081Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"light + collectDiscardedText\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/light-discarded-sentences\")",
   "id": "f25870082b690364",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [01:22<00:00, 20.60s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.332743Z",
     "start_time": "2025-03-11T04:36:48.123858Z"
    }
   },
   "cell_type": "code",
   "source": "documents_output[\"light + collectDiscardedText (paragraphs)\"] = compute_statistics(INPUT_CORPORA_PDFALTO, f\"{INPUT_CORPORA_GROBID}/light-discarded-paragraphs\")",
   "id": "4ef1d58ef2346461",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following corpus directories for PDFAlto: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories for Grobid: ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n",
      "Found the following corpus directories:  ['PLOS_1000', 'eLife_984', 'biorxiv-10k-test-2000', 'PMC_sample_1943']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing corpora: 100%|██████████| 4/4 [00:56<00:00, 14.07s/corpus]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: PLOS_1000, Processed 1000, skipped: 0\n",
      "Corpus: eLife_984, Processed 984, skipped: 0\n",
      "Corpus: biorxiv-10k-test-2000, Processed 2000, skipped: 0\n",
      "Corpus: PMC_sample_1943, Processed 1943, skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.342345Z",
     "start_time": "2025-03-11T04:37:45.273485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_averages(documents_):\n",
    "    for process_type, process in documents_.items():\n",
    "        for corpus, documents in process.items():\n",
    "            total_pdfalto = sum([d['pdfalto'] for d in documents['processed'].values()])\n",
    "            total_grobid = sum([d['grobid'] for d in documents['processed'].values()])\n",
    "            total_diff = sum([d['stats']['diff'] for d in documents['processed'].values()])\n",
    "            total_perc = sum([d['stats']['perc'] for d in documents['processed'].values()])\n",
    "            total_diff_tokens = sum([d['stats']['diff_tokens'] for d in documents['processed'].values()])\n",
    "            total_perc_tokens = sum([d['stats']['perc_tokens'] for d in documents['processed'].values()])\n",
    "\n",
    "            documents['average']['pdfalto'] = total_pdfalto / len(documents['processed'])\n",
    "            documents['average']['grobid'] = total_grobid / len(documents['processed'])\n",
    "            documents['average']['diff'] = total_diff / len(documents['processed'])\n",
    "            documents['average']['diff_tokens'] = total_diff_tokens / len(documents['processed'])\n",
    "            documents['average']['perc'] = total_perc / len(documents['processed'])\n",
    "            documents['average']['perc_tokens'] = total_perc_tokens / len(documents['processed'])\n",
    "\n",
    "        # data.append(documents[corpus]['average']['perc'])\n",
    "\n",
    "        # print(f\"\\t- Processed: {len(documents[corpus]['processed'])}, Skipped: {len(documents[corpus]['skipped'])}\")\n",
    "        # print(f\"\\t- Total length from PDFAlto: {total_pdfalto}\")\n",
    "        # print(f\"\\t- Total length from Grobid: {total_grobid}\")\n",
    "        # print(f\"\\t- Total Diff: {total_diff}\")\n",
    "        # print(f\"\\t- Total Perc: {documents[corpus]['average']['perc']}\")\n",
    "    return documents_\n",
    "\n",
    "\n",
    "documents_output_with_average = compute_averages(documents_output)\n"
   ],
   "id": "27c3e979c72d58b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results",
   "id": "54ad98611e460b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Character level evaluation\n",
    "#### Average percentage difference in characters"
   ],
   "id": "92922999340648a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.362328Z",
     "start_time": "2025-03-11T04:37:46.796403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "columns = [\"Process type\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output_with_average.items():\n",
    "    process_data = [process_type]\n",
    "    aggregated_data.append(process_data)\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['perc'])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(aggregated_data, columns=columns)\n",
    "results['character_level'] = df\n",
    "\n",
    "df\n"
   ],
   "id": "68f4408613938a8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   Process type  PLOS_1000  eLife_984  \\\n",
       "0                                      standard  11.821514   9.639087   \n",
       "1                         standard (paragraphs)  11.820248   9.633689   \n",
       "2                                         light  12.459632   6.857511   \n",
       "3                            light (paragraphs)  12.459843   6.856247   \n",
       "4               standard + collectDiscardedText   8.966189   4.817297   \n",
       "5  standard + collectDiscardedText (paragraphs)   8.964434   4.815603   \n",
       "6                  light + collectDiscardedText   3.174446   3.333342   \n",
       "7     light + collectDiscardedText (paragraphs)   3.174235   3.327930   \n",
       "\n",
       "   biorxiv-10k-test-2000  PMC_sample_1943  \n",
       "0               9.269665         6.451962  \n",
       "1               9.230569         6.450445  \n",
       "2               8.837437         8.516626  \n",
       "3               8.800080         8.515261  \n",
       "4               3.148627         4.103599  \n",
       "5               3.104529         4.102097  \n",
       "6               4.081236         2.133620  \n",
       "7               4.039645         2.132294  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process type</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>11.821514</td>\n",
       "      <td>9.639087</td>\n",
       "      <td>9.269665</td>\n",
       "      <td>6.451962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard (paragraphs)</td>\n",
       "      <td>11.820248</td>\n",
       "      <td>9.633689</td>\n",
       "      <td>9.230569</td>\n",
       "      <td>6.450445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light</td>\n",
       "      <td>12.459632</td>\n",
       "      <td>6.857511</td>\n",
       "      <td>8.837437</td>\n",
       "      <td>8.516626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>light (paragraphs)</td>\n",
       "      <td>12.459843</td>\n",
       "      <td>6.856247</td>\n",
       "      <td>8.800080</td>\n",
       "      <td>8.515261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standard + collectDiscardedText</td>\n",
       "      <td>8.966189</td>\n",
       "      <td>4.817297</td>\n",
       "      <td>3.148627</td>\n",
       "      <td>4.103599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standard + collectDiscardedText (paragraphs)</td>\n",
       "      <td>8.964434</td>\n",
       "      <td>4.815603</td>\n",
       "      <td>3.104529</td>\n",
       "      <td>4.102097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>light + collectDiscardedText</td>\n",
       "      <td>3.174446</td>\n",
       "      <td>3.333342</td>\n",
       "      <td>4.081236</td>\n",
       "      <td>2.133620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>light + collectDiscardedText (paragraphs)</td>\n",
       "      <td>3.174235</td>\n",
       "      <td>3.327930</td>\n",
       "      <td>4.039645</td>\n",
       "      <td>2.132294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Average characters length difference",
   "id": "7162f8715d53b38e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.394093Z",
     "start_time": "2025-03-11T04:37:47.934192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Process type\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output_with_average.items():\n",
    "    process_data = [process_type]\n",
    "    aggregated_data.append(process_data)\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['diff'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(aggregated_data, columns=columns)\n",
    "results['character_level_diff'] = df\n",
    "\n",
    "df"
   ],
   "id": "6290ffccf7a4c38c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   Process type  PLOS_1000    eLife_984  \\\n",
       "0                                      standard   5042.305  7621.560976   \n",
       "1                         standard (paragraphs)   5041.687  7619.377033   \n",
       "2                                         light   4893.161  5109.507114   \n",
       "3                            light (paragraphs)   4893.237  5108.392276   \n",
       "4               standard + collectDiscardedText   3844.632  3867.505081   \n",
       "5  standard + collectDiscardedText (paragraphs)   3844.061  3866.181911   \n",
       "6                  light + collectDiscardedText   1647.508  2568.152439   \n",
       "7     light + collectDiscardedText (paragraphs)   1647.440  2564.443089   \n",
       "\n",
       "   biorxiv-10k-test-2000  PMC_sample_1943  \n",
       "0              4870.3950      2533.489449  \n",
       "1              4861.6145      2532.909933  \n",
       "2              4026.2660      2988.691199  \n",
       "3              4015.9255      2988.205867  \n",
       "4              1654.9635      1669.830160  \n",
       "5              1644.5500      1669.277921  \n",
       "6              2011.3505       906.528049  \n",
       "7              2000.9100       906.061245  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process type</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>5042.305</td>\n",
       "      <td>7621.560976</td>\n",
       "      <td>4870.3950</td>\n",
       "      <td>2533.489449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard (paragraphs)</td>\n",
       "      <td>5041.687</td>\n",
       "      <td>7619.377033</td>\n",
       "      <td>4861.6145</td>\n",
       "      <td>2532.909933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light</td>\n",
       "      <td>4893.161</td>\n",
       "      <td>5109.507114</td>\n",
       "      <td>4026.2660</td>\n",
       "      <td>2988.691199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>light (paragraphs)</td>\n",
       "      <td>4893.237</td>\n",
       "      <td>5108.392276</td>\n",
       "      <td>4015.9255</td>\n",
       "      <td>2988.205867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standard + collectDiscardedText</td>\n",
       "      <td>3844.632</td>\n",
       "      <td>3867.505081</td>\n",
       "      <td>1654.9635</td>\n",
       "      <td>1669.830160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standard + collectDiscardedText (paragraphs)</td>\n",
       "      <td>3844.061</td>\n",
       "      <td>3866.181911</td>\n",
       "      <td>1644.5500</td>\n",
       "      <td>1669.277921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>light + collectDiscardedText</td>\n",
       "      <td>1647.508</td>\n",
       "      <td>2568.152439</td>\n",
       "      <td>2011.3505</td>\n",
       "      <td>906.528049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>light + collectDiscardedText (paragraphs)</td>\n",
       "      <td>1647.440</td>\n",
       "      <td>2564.443089</td>\n",
       "      <td>2000.9100</td>\n",
       "      <td>906.061245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Token level evaluation\n",
    "#### Average percentage difference in tokens\n"
   ],
   "id": "96fa4ec4dad5df2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.403710Z",
     "start_time": "2025-03-11T04:37:48.051748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Process type\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output.items():\n",
    "    process_data = [process_type]\n",
    "    aggregated_data.append(process_data)\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['perc_tokens'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(aggregated_data, columns=columns)\n",
    "\n",
    "results['token_level'] = df\n",
    "\n",
    "df\n"
   ],
   "id": "1abe5b56885a9f68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   Process type  PLOS_1000  eLife_984  \\\n",
       "0                                      standard   8.079996   5.460726   \n",
       "1                         standard (paragraphs)   8.073511   5.454855   \n",
       "2                                         light   8.931686   2.749526   \n",
       "3                            light (paragraphs)   8.932267   2.748054   \n",
       "4               standard + collectDiscardedText   5.163359  -0.164360   \n",
       "5  standard + collectDiscardedText (paragraphs)   5.156289  -0.167808   \n",
       "6                  light + collectDiscardedText  -0.167090  -0.529563   \n",
       "7     light + collectDiscardedText (paragraphs)  -0.166914  -0.535557   \n",
       "\n",
       "   biorxiv-10k-test-2000  PMC_sample_1943  \n",
       "0               7.451197         3.640059  \n",
       "1               7.377015         3.639200  \n",
       "2               5.663440         5.121684  \n",
       "3               5.591514         5.125644  \n",
       "4               0.702553         0.998441  \n",
       "5               0.619746         0.997513  \n",
       "6               1.385790        -0.797124  \n",
       "7               1.310566        -0.793043  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process type</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>8.079996</td>\n",
       "      <td>5.460726</td>\n",
       "      <td>7.451197</td>\n",
       "      <td>3.640059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard (paragraphs)</td>\n",
       "      <td>8.073511</td>\n",
       "      <td>5.454855</td>\n",
       "      <td>7.377015</td>\n",
       "      <td>3.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light</td>\n",
       "      <td>8.931686</td>\n",
       "      <td>2.749526</td>\n",
       "      <td>5.663440</td>\n",
       "      <td>5.121684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>light (paragraphs)</td>\n",
       "      <td>8.932267</td>\n",
       "      <td>2.748054</td>\n",
       "      <td>5.591514</td>\n",
       "      <td>5.125644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standard + collectDiscardedText</td>\n",
       "      <td>5.163359</td>\n",
       "      <td>-0.164360</td>\n",
       "      <td>0.702553</td>\n",
       "      <td>0.998441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standard + collectDiscardedText (paragraphs)</td>\n",
       "      <td>5.156289</td>\n",
       "      <td>-0.167808</td>\n",
       "      <td>0.619746</td>\n",
       "      <td>0.997513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>light + collectDiscardedText</td>\n",
       "      <td>-0.167090</td>\n",
       "      <td>-0.529563</td>\n",
       "      <td>1.385790</td>\n",
       "      <td>-0.797124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>light + collectDiscardedText (paragraphs)</td>\n",
       "      <td>-0.166914</td>\n",
       "      <td>-0.535557</td>\n",
       "      <td>1.310566</td>\n",
       "      <td>-0.793043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Average tokens length difference\n",
   "id": "9091953c9d78037c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.435510Z",
     "start_time": "2025-03-11T04:37:48.578299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"Process type\"]\n",
    "aggregated_data = []\n",
    "for process_type, process in documents_output.items():\n",
    "    process_data = [process_type]\n",
    "    aggregated_data.append(process_data)\n",
    "    for corpus, documents in process.items():\n",
    "        if corpus not in columns:\n",
    "            columns.append(corpus)\n",
    "        process_data.append(documents['average']['diff_tokens'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(aggregated_data, columns=columns)\n",
    "\n",
    "results['token_level_diff'] = df\n",
    "df"
   ],
   "id": "e8d28ff06d29435b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   Process type  PLOS_1000   eLife_984  \\\n",
       "0                                      standard    562.686  788.930894   \n",
       "1                         standard (paragraphs)    562.103  788.402439   \n",
       "2                                         light    552.945  345.103659   \n",
       "3                            light (paragraphs)    552.990  344.844512   \n",
       "4               standard + collectDiscardedText    346.402   11.170732   \n",
       "5  standard + collectDiscardedText (paragraphs)    345.828   10.676829   \n",
       "6                  light + collectDiscardedText     10.675  -66.486789   \n",
       "7     light + collectDiscardedText (paragraphs)     10.695  -67.210366   \n",
       "\n",
       "   biorxiv-10k-test-2000  PMC_sample_1943  \n",
       "0               735.1820       251.120947  \n",
       "1               730.7985       251.030365  \n",
       "2               412.3385       294.220278  \n",
       "3               407.7330       294.476068  \n",
       "4                40.2450        71.429748  \n",
       "5                35.2470        71.339166  \n",
       "6                60.0085       -53.407617  \n",
       "7                55.3690       -53.145651  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process type</th>\n",
       "      <th>PLOS_1000</th>\n",
       "      <th>eLife_984</th>\n",
       "      <th>biorxiv-10k-test-2000</th>\n",
       "      <th>PMC_sample_1943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standard</td>\n",
       "      <td>562.686</td>\n",
       "      <td>788.930894</td>\n",
       "      <td>735.1820</td>\n",
       "      <td>251.120947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standard (paragraphs)</td>\n",
       "      <td>562.103</td>\n",
       "      <td>788.402439</td>\n",
       "      <td>730.7985</td>\n",
       "      <td>251.030365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light</td>\n",
       "      <td>552.945</td>\n",
       "      <td>345.103659</td>\n",
       "      <td>412.3385</td>\n",
       "      <td>294.220278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>light (paragraphs)</td>\n",
       "      <td>552.990</td>\n",
       "      <td>344.844512</td>\n",
       "      <td>407.7330</td>\n",
       "      <td>294.476068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standard + collectDiscardedText</td>\n",
       "      <td>346.402</td>\n",
       "      <td>11.170732</td>\n",
       "      <td>40.2450</td>\n",
       "      <td>71.429748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standard + collectDiscardedText (paragraphs)</td>\n",
       "      <td>345.828</td>\n",
       "      <td>10.676829</td>\n",
       "      <td>35.2470</td>\n",
       "      <td>71.339166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>light + collectDiscardedText</td>\n",
       "      <td>10.675</td>\n",
       "      <td>-66.486789</td>\n",
       "      <td>60.0085</td>\n",
       "      <td>-53.407617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>light + collectDiscardedText (paragraphs)</td>\n",
       "      <td>10.695</td>\n",
       "      <td>-67.210366</td>\n",
       "      <td>55.3690</td>\n",
       "      <td>-53.145651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.445129Z",
     "start_time": "2025-03-11T04:37:49.884507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "cl = results['character_level']\n",
    "cl_markdown = cl.to_markdown(index=False)\n",
    "tl = results['token_level']\n",
    "tl_markdown = tl.to_markdown(index=False)\n",
    "\n",
    "# Get current date and time\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create filename with date and time\n",
    "filename = f\"output_{current_time}.md\"\n",
    "\n",
    "# Save markdown table to file\n",
    "with open(os.path.join(\"results\", filename), \"w\") as file:\n",
    "    file.write(\"# Characters level evaluation\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(cl_markdown)\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(\"# Tokens level evaluation\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    file.write(tl_markdown)\n"
   ],
   "id": "4f5927a5c231c54d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysis specific documents",
   "id": "201380166577b233"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.449410Z",
     "start_time": "2025-03-11T04:37:51.159416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for process_type, process in documents_output.items():\n",
    "    print(process_type)\n",
    "    for corpus, documents in process.items():\n",
    "        print(f\"\\t{corpus}\")\n",
    "        sorted_by_diff = sorted(\n",
    "            documents['processed'].items(),\n",
    "            key=lambda x: x[1]['stats']['diff']\n",
    "        )\n",
    "\n",
    "        sorted_by_diff_tokens = sorted(\n",
    "            documents['processed'].items(),\n",
    "            key=lambda x: x[1]['stats']['diff_tokens']\n",
    "        )\n",
    "\n",
    "        output_neg = [f\"\\n\\t\\t\\t -{item[0]}: {item[1]['stats']['diff']}\" for item in sorted_by_diff[:2]]\n",
    "        print(\"\".join(output_neg))\n",
    "        output_pos = [f\"\\n\\t\\t\\t -{item[0]}: {item[1]['stats']['diff']}\" for item in sorted_by_diff[-2:]]\n",
    "        print(\"\".join(output_pos))\n",
    "        # print(f\"Files to check:\\n {[item[0] for item in sorted_by_diff_tokens[:2]]}\")\n"
   ],
   "id": "f4da94821bc2d8e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pgen.1002782/pgen.1002782: -9513\n",
      "\t\t\t -pone.0002784/pone.0002784: -4406\n",
      "\n",
      "\t\t\t -pone.0270278/pone.0270278: 19209\n",
      "\t\t\t -pone.0278380/pone.0278380: 19477\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -83628/elife-83628-v1: -19977\n",
      "\t\t\t -63910/elife-63910-v2: -5693\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 71005\n",
      "\t\t\t -73679/elife-73679-v2: 112692\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -43153\n",
      "\t\t\t -030122v1/030122v1: -5648\n",
      "\n",
      "\t\t\t -413708v1/413708v1: 56583\n",
      "\t\t\t -454355v1/454355v1: 96222\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Mucosal_Immunol_2011_Jul_30_4(4)_468-478/mi20118a: -4401\n",
      "\t\t\t -Protein_Sci_2010_Nov_10_19(11)_2131-2140/pro0019-2131: -3903\n",
      "\n",
      "\t\t\t -Mol_Phylogenet_Evol_2010_Oct_57(1)_266-284/main: 21114\n",
      "\t\t\t -J_Adv_Nurs_2011_Feb_67(2)_228-250/jan0067-0228: 23014\n",
      "standard (paragraphs)\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pgen.1002782/pgen.1002782: -9513\n",
      "\t\t\t -pone.0002784/pone.0002784: -4406\n",
      "\n",
      "\t\t\t -pone.0270278/pone.0270278: 19209\n",
      "\t\t\t -pone.0278380/pone.0278380: 19477\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -83628/elife-83628-v1: -20000\n",
      "\t\t\t -63910/elife-63910-v2: -5693\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 71005\n",
      "\t\t\t -73679/elife-73679-v2: 112692\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -43153\n",
      "\t\t\t -030122v1/030122v1: -5648\n",
      "\n",
      "\t\t\t -413708v1/413708v1: 56583\n",
      "\t\t\t -454355v1/454355v1: 96222\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Mucosal_Immunol_2011_Jul_30_4(4)_468-478/mi20118a: -4412\n",
      "\t\t\t -Protein_Sci_2010_Nov_10_19(11)_2131-2140/pro0019-2131: -3904\n",
      "\n",
      "\t\t\t -Mol_Phylogenet_Evol_2010_Oct_57(1)_266-284/main: 21114\n",
      "\t\t\t -J_Adv_Nurs_2011_Feb_67(2)_228-250/jan0067-0228: 23014\n",
      "light\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pbio.0040278/pbio.0040278: 121\n",
      "\t\t\t -pbio.0050278/pbio.0050278: 152\n",
      "\n",
      "\t\t\t -pone.0278174/pone.0278174: 11966\n",
      "\t\t\t -pone.0278185/pone.0278185: 12626\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -59147/elife-59147-v1: -101\n",
      "\t\t\t -28339/elife-28339-v1: 55\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 55221\n",
      "\t\t\t -73679/elife-73679-v2: 55953\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -46075\n",
      "\t\t\t -265959v1/265959v1: -303\n",
      "\n",
      "\t\t\t -110353v1/110353v1: 40122\n",
      "\t\t\t -454355v1/454355v1: 201355\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Exp_Physiol_2011_Feb_96(2)_71-72/eph0096-0071: 1\n",
      "\t\t\t -Ethik_Med_2010_Mar_22_22(1)_55-57/481_2009_Article_51: 93\n",
      "\n",
      "\t\t\t -Biol_Rev_Camb_Philos_Soc_2011_Feb_86(1)_117-155/brv0086-0117: 13870\n",
      "\t\t\t -Crit_Rev_Toxicol_2011_Mar_14_41(3)_230-268/btxc41-230: 19050\n",
      "light (paragraphs)\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pbio.0040278/pbio.0040278: 121\n",
      "\t\t\t -pbio.0050278/pbio.0050278: 152\n",
      "\n",
      "\t\t\t -pone.0278174/pone.0278174: 11966\n",
      "\t\t\t -pone.0278185/pone.0278185: 12626\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -59147/elife-59147-v1: -101\n",
      "\t\t\t -28339/elife-28339-v1: 55\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 55221\n",
      "\t\t\t -73679/elife-73679-v2: 55953\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -46090\n",
      "\t\t\t -265959v1/265959v1: -303\n",
      "\n",
      "\t\t\t -110353v1/110353v1: 40122\n",
      "\t\t\t -454355v1/454355v1: 201355\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Exp_Physiol_2011_Feb_96(2)_71-72/eph0096-0071: 1\n",
      "\t\t\t -Ethik_Med_2010_Mar_22_22(1)_55-57/481_2009_Article_51: 93\n",
      "\n",
      "\t\t\t -Biol_Rev_Camb_Philos_Soc_2011_Feb_86(1)_117-155/brv0086-0117: 13870\n",
      "\t\t\t -Crit_Rev_Toxicol_2011_Mar_14_41(3)_230-268/btxc41-230: 19050\n",
      "standard + collectDiscardedText\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pgen.1002782/pgen.1002782: -10055\n",
      "\t\t\t -pone.0278972/pone.0278972: -7736\n",
      "\n",
      "\t\t\t -pone.0278032/pone.0278032: 15405\n",
      "\t\t\t -pone.0278380/pone.0278380: 18586\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -83628/elife-83628-v1: -32147\n",
      "\t\t\t -00808/elife-00808-v1: -13752\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 55556\n",
      "\t\t\t -73679/elife-73679-v2: 110557\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -49255\n",
      "\t\t\t -262600v1/262600v1: -33143\n",
      "\n",
      "\t\t\t -249573v1/249573v1: 29799\n",
      "\t\t\t -454355v1/454355v1: 39293\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Eur_J_Med_Chem_2011_Jun_46(6)_2091-2101/main: -15253\n",
      "\t\t\t -Stud_Mycol_2011_68_79-113/0079: -14343\n",
      "\n",
      "\t\t\t -Mol_Phylogenet_Evol_2010_Oct_57(1)_266-284/main: 20722\n",
      "\t\t\t -J_Adv_Nurs_2011_Feb_67(2)_228-250/jan0067-0228: 22183\n",
      "standard + collectDiscardedText (paragraphs)\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pgen.1002782/pgen.1002782: -10055\n",
      "\t\t\t -pone.0278972/pone.0278972: -7736\n",
      "\n",
      "\t\t\t -pone.0278032/pone.0278032: 15405\n",
      "\t\t\t -pone.0278380/pone.0278380: 18586\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -83628/elife-83628-v1: -32185\n",
      "\t\t\t -00808/elife-00808-v1: -13752\n",
      "\n",
      "\t\t\t -36495/elife-36495-v1: 55556\n",
      "\t\t\t -73679/elife-73679-v2: 110557\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -49255\n",
      "\t\t\t -262600v1/262600v1: -33143\n",
      "\n",
      "\t\t\t -249573v1/249573v1: 29799\n",
      "\t\t\t -454355v1/454355v1: 39293\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Eur_J_Med_Chem_2011_Jun_46(6)_2091-2101/main: -15253\n",
      "\t\t\t -Stud_Mycol_2011_68_79-113/0079: -14343\n",
      "\n",
      "\t\t\t -Mol_Phylogenet_Evol_2010_Oct_57(1)_266-284/main: 20722\n",
      "\t\t\t -J_Adv_Nurs_2011_Feb_67(2)_228-250/jan0067-0228: 22183\n",
      "light + collectDiscardedText\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pone.0278474/pone.0278474: -251\n",
      "\t\t\t -pone.0278017/pone.0278017: -226\n",
      "\n",
      "\t\t\t -pone.0278918/pone.0278918: 7253\n",
      "\t\t\t -pone.0270278/pone.0270278: 7804\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -11469/elife-11469-v2: -1908\n",
      "\t\t\t -61771/elife-61771-v2: -468\n",
      "\n",
      "\t\t\t -73679/elife-73679-v2: 21473\n",
      "\t\t\t -06416/elife-06416-v1: 21600\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -47571\n",
      "\t\t\t -172015v1/172015v1: -493\n",
      "\n",
      "\t\t\t -454355v1/454355v1: 14560\n",
      "\t\t\t -145334v1/145334v1: 18274\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Int_J_Pept_Res_Ther_2010_Sep_26_16(3)_199-213/10989_2010_Article_9230: -1572\n",
      "\t\t\t -J_Cardiovasc_Magn_Reson_2011_May_3_13(1)_26/1532-429X-13-26: -986\n",
      "\n",
      "\t\t\t -Bioanal_Rev_2010_Dec_21_2(1-4)_23-60/12566_2010_Article_15: 9734\n",
      "\t\t\t -CNS_Neurol_Disord_Drug_Targets_2010_Aug_9(4)_504-523/CDTCNSND-9-504: 10759\n",
      "light + collectDiscardedText (paragraphs)\n",
      "\tPLOS_1000\n",
      "\n",
      "\t\t\t -pone.0278474/pone.0278474: -251\n",
      "\t\t\t -pone.0278017/pone.0278017: -226\n",
      "\n",
      "\t\t\t -pone.0278918/pone.0278918: 7253\n",
      "\t\t\t -pone.0270278/pone.0270278: 7804\n",
      "\teLife_984\n",
      "\n",
      "\t\t\t -11469/elife-11469-v2: -1908\n",
      "\t\t\t -61771/elife-61771-v2: -468\n",
      "\n",
      "\t\t\t -73679/elife-73679-v2: 21473\n",
      "\t\t\t -06416/elife-06416-v1: 21600\n",
      "\tbiorxiv-10k-test-2000\n",
      "\n",
      "\t\t\t -392563v1/392563v1: -47586\n",
      "\t\t\t -172015v1/172015v1: -493\n",
      "\n",
      "\t\t\t -454355v1/454355v1: 14560\n",
      "\t\t\t -145334v1/145334v1: 18274\n",
      "\tPMC_sample_1943\n",
      "\n",
      "\t\t\t -Int_J_Pept_Res_Ther_2010_Sep_26_16(3)_199-213/10989_2010_Article_9230: -1575\n",
      "\t\t\t -J_Cardiovasc_Magn_Reson_2011_May_3_13(1)_26/1532-429X-13-26: -986\n",
      "\n",
      "\t\t\t -Bioanal_Rev_2010_Dec_21_2(1-4)_23-60/12566_2010_Article_15: 9734\n",
      "\t\t\t -CNS_Neurol_Disord_Drug_Targets_2010_Aug_9(4)_504-523/CDTCNSND-9-504: 10759\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.457432Z",
     "start_time": "2025-03-11T04:37:52.898348Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "51347dcefdca0c0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T05:50:41.478003Z",
     "start_time": "2025-03-11T04:37:53.298153Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b5b138538203e368",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
